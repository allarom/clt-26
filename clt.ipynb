{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuyR1KVpq5db"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYT3YqSG-mFd",
        "outputId": "642fb1e2-a5da-469e-a51a-234235204883"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: unidecode in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (1.4.0)\n",
            "Requirement already satisfied: spacy in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (0.24.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (4.67.3)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (2.12.5)\n",
            "Requirement already satisfied: jinja2 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (80.10.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.5 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.5)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: typer>=0.24.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (0.24.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.1)\n",
            "Requirement already satisfied: wrapt in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: click>=8.2.1 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=12.3.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.0.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.2.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich>=12.3.0->typer>=0.24.0->typer-slim<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages (from jinja2->spacy) (3.0.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "2QhtVEU6AjlD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import re       # regex expressions\n",
        "import ast      # for tag list\n",
        "import unidecode  # replacement of accented characters\n",
        "import spacy\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eNPomRbIbJF"
      },
      "source": [
        "# Stage 1: Enhanced Data Cleaning, Preprocessing, and Exploratory Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5cGZ9JnIp-o"
      },
      "source": [
        "## 1.1 Data Collection & Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xamkwk1nTIa"
      },
      "source": [
        "*Done by: Marisa  - Checked by: Alla*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment setup\n",
        "Switch: \"local\" (VS Code + GitHub) | \"colab\" (Google Colab + Drive)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENV = \"local\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knVwSfcqK9tA"
      },
      "source": [
        "### Data Import\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdDWQ62qbVp7"
      },
      "source": [
        "- In your Google Drive, create a folder \"CLT\" and upload the csv with its original name (downloaded from Kaggle)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGzhMwM4II_Y",
        "outputId": "a7319f85-e965-4680-dbc6-5eb8223d1dce"
      },
      "outputs": [],
      "source": [
        "if ENV == \"colab\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "zXmmXfryLgT4",
        "outputId": "d91d365a-601b-4e37-8d5b-e788cee0d0ba"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "      <th>domain</th>\n",
              "      <th>url</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88571</td>\n",
              "      <td>Ricoh to provide customer support for Agility ...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['The Digit humanoid could work in distributio...</td>\n",
              "      <td>therobotreport</td>\n",
              "      <td>https://www.therobotreport.com/ricoh-provides-...</td>\n",
              "      <td>['Robotics', 'Video', 'BostonDynamics']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>92022</td>\n",
              "      <td>MTV VMAs 2024: Live shopping is coming to the ...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['In this article', \"When viewers tune in to t...</td>\n",
              "      <td>cnbc</td>\n",
              "      <td>https://www.cnbc.com/2024/09/11/mtv-vmas-2024-...</td>\n",
              "      <td>['GenerativeAI']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>81522</td>\n",
              "      <td>Open-source imagery is transforming investigat...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['Open-source online imagery can play a vital ...</td>\n",
              "      <td>theconversation</td>\n",
              "      <td>https://www.theconversation.com/open-source-im...</td>\n",
              "      <td>['Disinformation', 'Video', 'Deepfake', 'Infor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>81521</td>\n",
              "      <td>With China seeking AI dominance, Taiwan’ s eff...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['Tensions between China, Taiwan and the U.S. ...</td>\n",
              "      <td>theconversation</td>\n",
              "      <td>https://www.theconversation.com/with-china-see...</td>\n",
              "      <td>['MilitaryAndDefense', 'Missiles', 'HighPerfor...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>63808</td>\n",
              "      <td>Study links EV charging stations to increased ...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['Countries globally are rapidly transitioning...</td>\n",
              "      <td>phys</td>\n",
              "      <td>https://phys.org/news/2024-09-links-ev-station...</td>\n",
              "      <td>['Causality', 'Planning']</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0                                              title        date  \\\n",
              "0       88571  Ricoh to provide customer support for Agility ...  2024-09-11   \n",
              "1       92022  MTV VMAs 2024: Live shopping is coming to the ...  2024-09-11   \n",
              "2       81522  Open-source imagery is transforming investigat...  2024-09-11   \n",
              "3       81521  With China seeking AI dominance, Taiwan’ s eff...  2024-09-11   \n",
              "4       63808  Study links EV charging stations to increased ...  2024-09-11   \n",
              "\n",
              "                                             content           domain  \\\n",
              "0  ['The Digit humanoid could work in distributio...   therobotreport   \n",
              "1  ['In this article', \"When viewers tune in to t...             cnbc   \n",
              "2  ['Open-source online imagery can play a vital ...  theconversation   \n",
              "3  ['Tensions between China, Taiwan and the U.S. ...  theconversation   \n",
              "4  ['Countries globally are rapidly transitioning...             phys   \n",
              "\n",
              "                                                 url  \\\n",
              "0  https://www.therobotreport.com/ricoh-provides-...   \n",
              "1  https://www.cnbc.com/2024/09/11/mtv-vmas-2024-...   \n",
              "2  https://www.theconversation.com/open-source-im...   \n",
              "3  https://www.theconversation.com/with-china-see...   \n",
              "4  https://phys.org/news/2024-09-links-ev-station...   \n",
              "\n",
              "                                                tags  \n",
              "0            ['Robotics', 'Video', 'BostonDynamics']  \n",
              "1                                   ['GenerativeAI']  \n",
              "2  ['Disinformation', 'Video', 'Deepfake', 'Infor...  \n",
              "3  ['MilitaryAndDefense', 'Missiles', 'HighPerfor...  \n",
              "4                          ['Causality', 'Planning']  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "if ENV == \"colab\":\n",
        "    csv_path = '/content/drive/My Drive/CLT/ai_media_dataset_20250911.csv'\n",
        "else:\n",
        "    csv_path = 'data/ai_media_dataset_20250911.csv'\n",
        "\n",
        "df = pd.read_csv(csv_path)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Od3x23Lrhl",
        "outputId": "e6ffdff3-8c8d-4aa8-bfa7-7ea06b57531f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16527, 7)"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncfoiYZYL-y3"
      },
      "source": [
        "The dataset contains 16'527 observations with 7 columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-iSD0ruzKp73"
      },
      "source": [
        "### Removing Duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGIKzVZZK5I0",
        "outputId": "10ef98d5-89fe-4dde-a343-12c2ee062e0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate rows: 0\n",
            "Number of duplicate titles: 0\n",
            "Number of duplicate content:9\n",
            "Number of duplicate URLs: 0\n"
          ]
        }
      ],
      "source": [
        "duplicate_rows = df[df.duplicated()]\n",
        "print(f\"Number of duplicate rows: {len(duplicate_rows)}\")\n",
        "\n",
        "title_duplicate = df[df.duplicated(subset='title')]\n",
        "print(f\"Number of duplicate titles: {len(title_duplicate)}\")\n",
        "\n",
        "content_duplicate = df[df.duplicated(subset ='content')]\n",
        "print(f\"Number of duplicate content:{len(content_duplicate)}\")\n",
        "\n",
        "url_duplicate = df[df.duplicated(subset ='url')]\n",
        "print(f\"Number of duplicate URLs: {len(url_duplicate)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xhcGYsGMgPQ"
      },
      "source": [
        "There are no duplicate rows, titles and urls. But there is duplicate content. We take a closer look at the duplicate content:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfNLuK9QNYKp",
        "outputId": "edb2d821-a128-46fc-af7f-481097bd261d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Unnamed: 0                                              title  \\\n",
            "920         93651  Performance, Security FOMO Could Fuel The AI P...   \n",
            "933         93649  A Performance, Security FOMO Could Fuel The AI...   \n",
            "6354       105429  ProMat 2025 show planner: Bigger and better th...   \n",
            "7070       105445  ProMat Show Planner: 2025 to be bigger and bet...   \n",
            "2582       105328           Maximizing automation at Outdoor Network   \n",
            "5174       105395    Maximizing automated systems at Outdoor Network   \n",
            "3300        94135  Connect the dots: how to use relationship netw...   \n",
            "9337        94172  Connecting the dots: how to use relationship n...   \n",
            "3914       105357   Object and pedestrian detection solutions evolve   \n",
            "5178       105394  Object and pedestrian detection warehouse safe...   \n",
            "528          3230      Gen AI-Powered Reinvention: How APAC can Lead   \n",
            "568          3232      Generative AI Powered Strategy in APAC Region   \n",
            "4422       105373  Supply Chain Imperatives 2024 and Beyond - Sup...   \n",
            "13669      105728  Supply Chain Imperatives 2024 and Beyond - Mat...   \n",
            "10354       74288  meta-llama/Llama-4-Maverick-17B-128E-Instruct ...   \n",
            "10451       74290  meta-llama/Llama-4-Scout-17B-16E-Instruct · Hu...   \n",
            "7570        90838                             Top NFT Companies 2025   \n",
            "7582        90840               Top United States NFT Companies 2025   \n",
            "\n",
            "             date                                            content  \\\n",
            "920    2024-10-03  [\"“ You have to be careful where you position ...   \n",
            "933    2024-10-03  [\"“ You have to be careful where you position ...   \n",
            "6354   2025-01-27  ['As a premier global event for manufacturing ...   \n",
            "7070   2025-02-01  ['As a premier global event for manufacturing ...   \n",
            "2582   2024-11-04  ['It can be reassuring to think a mix of wareh...   \n",
            "5174   2025-01-06  ['It can be reassuring to think a mix of wareh...   \n",
            "3300   2024-11-16  ['It may sound like an exaggeration, but there...   \n",
            "9337   2025-03-16  ['It may sound like an exaggeration, but there...   \n",
            "3914   2024-12-02  ['Object and pedestrian detection and avoidanc...   \n",
            "5178   2025-01-06  ['Object and pedestrian detection and avoidanc...   \n",
            "528    2024-09-25  ['Over the past four years, the APAC region ha...   \n",
            "568    2024-09-25  ['Over the past four years, the APAC region ha...   \n",
            "4422   2024-12-12  ['Supply Chain Solutions hold Commerce Togethe...   \n",
            "13669  2025-06-08  ['Supply Chain Solutions hold Commerce Togethe...   \n",
            "10354  2025-04-01  ['The information you provide will be collecte...   \n",
            "10451  2025-04-02  ['The information you provide will be collecte...   \n",
            "7570   2025-02-08  ['ZORA is a group of individuals working towar...   \n",
            "7582   2025-02-09  ['ZORA is a group of individuals working towar...   \n",
            "\n",
            "               domain                                                url  \\\n",
            "920               crn  https://www.crn.com/news/ai/2024/performance-s...   \n",
            "933               crn  https://www.crn.com/news/ai/2024/performance-s...   \n",
            "6354   supplychain247  https://www.supplychain247.com/article/promat_...   \n",
            "7070   supplychain247  https://www.supplychain247.com/article/promat_...   \n",
            "2582   supplychain247  https://www.supplychain247.com/article/maximiz...   \n",
            "5174   supplychain247  https://www.supplychain247.com/article/maximiz...   \n",
            "3300           avaloq  https://www.avaloq.com/resources/blog/connecti...   \n",
            "9337           avaloq  https://www.avaloq.com/fr/Insights/blog/connec...   \n",
            "3914   supplychain247  https://www.supplychain247.com/article/object_...   \n",
            "5178   supplychain247  https://www.supplychain247.com/article/object_...   \n",
            "528         accenture  https://www.accenture.com/hk-en/insights/strat...   \n",
            "568         accenture  https://www.accenture.com/bg-en/insights/strat...   \n",
            "4422   supplychain247  https://www.supplychain247.com/virtual-2024/pa...   \n",
            "13669  supplychain247  https://www.supplychain247.com/summit-2025/pap...   \n",
            "10354     huggingface  https://huggingface.co/meta-llama/Llama-4-Mave...   \n",
            "10451     huggingface  https://huggingface.co/meta-llama/Llama-4-Scou...   \n",
            "7570          builtin   https://builtin.com/companies/type/nft-companies   \n",
            "7582          builtin  https://builtin.com/companies/type/nft-compani...   \n",
            "\n",
            "                                                    tags  \n",
            "920    ['Planning', 'Video', 'LanguageModel', 'Genera...  \n",
            "933    ['Planning', 'Video', 'LanguageModel', 'Genera...  \n",
            "6354                            ['Robotics', 'Planning']  \n",
            "7070                            ['Robotics', 'Planning']  \n",
            "2582                            ['Robotics', 'Planning']  \n",
            "5174                            ['Robotics', 'Planning']  \n",
            "3300                ['GraphAnalytics', 'Accountability']  \n",
            "9337                ['GraphAnalytics', 'Accountability']  \n",
            "3914   ['Robotics', 'ObjectRecognition', 'GenerativeA...  \n",
            "5178   ['Robotics', 'ObjectRecognition', 'GenerativeA...  \n",
            "528    ['GenerativeAI', 'SyntheticData', 'Accountabil...  \n",
            "568    ['GenerativeAI', 'SyntheticData', 'Accountabil...  \n",
            "4422                                  ['Accountability']  \n",
            "13669                                 ['Accountability']  \n",
            "10354  ['HuggingFace', 'Llama', 'LargeLanguageModel',...  \n",
            "10451  ['HuggingFace', 'Llama', 'LargeLanguageModel',...  \n",
            "7570           ['Metaverse', 'Gaming', 'Accountability']  \n",
            "7582           ['Metaverse', 'Gaming', 'Accountability']  \n"
          ]
        }
      ],
      "source": [
        "duplicate_content_rows = df[df.duplicated(subset=['content'], keep=False)].sort_values(by='content')\n",
        "print(duplicate_content_rows)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T45dGaYEPzB3"
      },
      "source": [
        "As there are only nine duplicate rows, we can easily review the output above manually to identify the following patterns:\n",
        "\n",
        "* **Titles:** There are slight differences that do not seem relevant regarding content (for example, with and without an article, or with and without a year).\n",
        "* **Dates:** Publication dates vary, sometimes differing by several months.\n",
        "* **Domains:** The source domain remains consistently identical across all duplicates.\n",
        "* **URLs:** Links vary occasionally, typically reflecting different regional paths or language preferences.\n",
        "* **Tags:** The assigned tags are always identical.\n",
        "\n",
        "\n",
        "Since the duplicated content consistently originates from the same source domain, we have decided to retain only the earliest published instance of each text. This approach ensures we capture the relevant information precisely when it first emerged."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 310
        },
        "id": "Hr3gJvcOS-vt",
        "outputId": "bd7b7602-e4d6-4e2e-fa69-83253387d413"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>title</th>\n",
              "      <th>date</th>\n",
              "      <th>content</th>\n",
              "      <th>domain</th>\n",
              "      <th>url</th>\n",
              "      <th>tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88571</td>\n",
              "      <td>Ricoh to provide customer support for Agility ...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['The Digit humanoid could work in distributio...</td>\n",
              "      <td>therobotreport</td>\n",
              "      <td>https://www.therobotreport.com/ricoh-provides-...</td>\n",
              "      <td>['Robotics', 'Video', 'BostonDynamics']</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>70996</td>\n",
              "      <td>AI Design Trends to Watch in 2025: From Photor...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['Artificial Intelligence ( AI) is rapidly tra...</td>\n",
              "      <td>geeksforgeeks</td>\n",
              "      <td>https://www.geeksforgeeks.org/ai-design-trends...</td>\n",
              "      <td>['Personalisation', 'NaturalLanguageProcessing...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>104159</td>\n",
              "      <td>AIs generate more novel and exciting research ...</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['The first statistically significant results ...</td>\n",
              "      <td>newatlas</td>\n",
              "      <td>https://newatlas.com/technology/llm-novel-rese...</td>\n",
              "      <td>['LargeLanguageModel', 'ChatGPT', 'Anthropic',...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>73625</td>\n",
              "      <td>Argonne's HPC/AI User Forum Wrap Up</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['As fans of this publication will already kno...</td>\n",
              "      <td>hpcwire</td>\n",
              "      <td>https://www.hpcwire.com/2024/09/11/argonnes-hp...</td>\n",
              "      <td>['HighPerformanceComputing', 'GenerativeAI', '...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>80150</td>\n",
              "      <td>Top 10: Supply Chain Optimisation Strategies</td>\n",
              "      <td>2024-09-11</td>\n",
              "      <td>['Supply chain optimisation has never been mor...</td>\n",
              "      <td>supplychaindigital</td>\n",
              "      <td>https://www.supplychaindigital.com/top10/top-1...</td>\n",
              "      <td>['Finetuning', 'Traceable', 'Accountability', ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Unnamed: 0                                              title       date  \\\n",
              "0        88571  Ricoh to provide customer support for Agility ... 2024-09-11   \n",
              "26       70996  AI Design Trends to Watch in 2025: From Photor... 2024-09-11   \n",
              "27      104159  AIs generate more novel and exciting research ... 2024-09-11   \n",
              "28       73625                Argonne's HPC/AI User Forum Wrap Up 2024-09-11   \n",
              "29       80150       Top 10: Supply Chain Optimisation Strategies 2024-09-11   \n",
              "\n",
              "                                              content              domain  \\\n",
              "0   ['The Digit humanoid could work in distributio...      therobotreport   \n",
              "26  ['Artificial Intelligence ( AI) is rapidly tra...       geeksforgeeks   \n",
              "27  ['The first statistically significant results ...            newatlas   \n",
              "28  ['As fans of this publication will already kno...             hpcwire   \n",
              "29  ['Supply chain optimisation has never been mor...  supplychaindigital   \n",
              "\n",
              "                                                  url  \\\n",
              "0   https://www.therobotreport.com/ricoh-provides-...   \n",
              "26  https://www.geeksforgeeks.org/ai-design-trends...   \n",
              "27  https://newatlas.com/technology/llm-novel-rese...   \n",
              "28  https://www.hpcwire.com/2024/09/11/argonnes-hp...   \n",
              "29  https://www.supplychaindigital.com/top10/top-1...   \n",
              "\n",
              "                                                 tags  \n",
              "0             ['Robotics', 'Video', 'BostonDynamics']  \n",
              "26  ['Personalisation', 'NaturalLanguageProcessing...  \n",
              "27  ['LargeLanguageModel', 'ChatGPT', 'Anthropic',...  \n",
              "28  ['HighPerformanceComputing', 'GenerativeAI', '...  \n",
              "29  ['Finetuning', 'Traceable', 'Accountability', ...  "
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "df_sorted = df.sort_values(by='date', ascending=True)\n",
        "df_unique = df_sorted.drop_duplicates(subset = 'content', keep = 'first')\n",
        "\n",
        "df_unique.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vx9bvbrAUKlH",
        "outputId": "9f4700fa-1773-4251-e1fc-dd1e43751119"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(16518, 7)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_unique.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPlvnoZLUSQd",
        "outputId": "d9aa876f-ee29-49ab-c7ab-16f7ebd3a358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of duplicate content:0\n"
          ]
        }
      ],
      "source": [
        "df_unique_content_duplicate = df_unique[df_unique.duplicated(subset ='content')]\n",
        "print(f\"Number of duplicate content:{len(df_unique_content_duplicate)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OK2S-aFEUgGu"
      },
      "source": [
        "We successfully removed duplicate rows. Checking for shape confirmed that 9 rows were removed as well as checking for duplicate content again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hRb7cASDViPu"
      },
      "source": [
        "### Handling missing values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2zsScKeAVmCe"
      },
      "source": [
        "Firstly, we check for missing values. We double check with different methods:\n",
        "- **NA-Values**: Values that are actually missing and indicated as missing in the data\n",
        "- **Empty string**: Missing values might not be identified because they are empty strings.\n",
        "- **Placeholders**: Instead of an empty value, placeholders like \"N/A\" might be used. We check for the most common ones.\n",
        "- **Empty Lists**: Tags are saved as lists. We check for empty lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMrARPS_VowE",
        "outputId": "fecb6f79-b4e9-4171-eae7-2b353d378d91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of NA-values: 0\n",
            "Number of empty strings: 0\n",
            "Number of placeholders: 0\n",
            "Empty tags count: 0\n"
          ]
        }
      ],
      "source": [
        "# Check for NA-values\n",
        "missing_values_NA = df_unique.isna()\n",
        "print(f\"Number of NA-values: {missing_values_NA.sum().sum()}\")\n",
        "\n",
        "# Check for empty strings\n",
        "text_columns = df_unique.select_dtypes(include = ['object', 'string']).columns\n",
        "empty_strings = df_unique[text_columns].apply(lambda col: col.str.strip() == '')\n",
        "print(f\"Number of empty strings: {empty_strings.sum().sum()}\")\n",
        "\n",
        "# Check for placeholders\n",
        "placeholders = ['N/A', 'n/a', 'null', 'Null', 'none', 'None', 'unknown', 'Unknown', '-']\n",
        "placeholder_values = df_unique.isin(placeholders)\n",
        "print(f\"Number of placeholders: {placeholder_values.sum().sum()}\")\n",
        "\n",
        "# Check for empty lists\n",
        "empty_tags_count = (df_unique['tags'] == '[]').sum()\n",
        "print(\"Empty tags count:\", empty_tags_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZGMnRUgamWH"
      },
      "source": [
        "There are no missing values in this data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l19kOY4hbac_"
      },
      "source": [
        "### Normalize Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jvsObmicCbS"
      },
      "source": [
        "We create a function that cleans the text in regards of\n",
        "- **Removal of html tags**: All content within brackets < > is removed to strip away any residual web formatting.\n",
        "- **Replacing accented characters**: We use the unidecode library to replace all characters with accents (e.g., \"é\", \"ö\") with their closest ASCII equivalent (\"e\", \"o\") for better consistency.\n",
        "- **Casing**: We transform the entire text to lower-case letters to ensure that words like \"Tech\" and \"tech\" are treated as the same token.\n",
        "- **Removal of emojis**: WW remove all emoji-like patterns and special Unicode symbols that do not add semantic value to the text analysis.\n",
        "- **Removal of irrelevant symbols**: We delete all symbols that are not letters, numbers, or spaces, replacing them with a single space to prevent words from merging.\n",
        "- **Removal of white space**: We collapse multiple spaces into one and strip leading or trailing white spaces from the text.\n",
        "\n",
        "For this, we use Regex expressions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "rGeyA4q-cx77"
      },
      "outputs": [],
      "source": [
        "def clean_text(text):\n",
        "    # Check if value is text\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    # HTML-tags\n",
        "    text = re.sub(r'<[^>]+>', '', text)\n",
        "\n",
        "    # Accented characters\n",
        "    text = unidecode.unidecode(text)\n",
        "\n",
        "    # Casing\n",
        "    text = text.lower()\n",
        "\n",
        "    # Emoji patterns\n",
        "    emojis_pattern = re.compile(pattern=\"[\"\n",
        "                        u\"\\U0001F600-\\U0001F64F\"\n",
        "                        u\"\\U0001F300-\\U0001F5FF\"\n",
        "                        u\"\\U0001F680-\\U0001F6FF\"\n",
        "                        u\"\\U0001F1E0-\\U0001F1FF\"\n",
        "                        u\"\\U00002500-\\U00002BEF\"\n",
        "                        u\"\\U00002702-\\U000027B0\"\n",
        "                        u\"\\U000024C2-\\U0001F251\"\n",
        "                        u\"\\U0001f926-\\U0001f937\"\n",
        "                        u\"\\U00010000-\\U0010ffff\"\n",
        "                        u\"\\u2640-\\u2642\"\n",
        "                        u\"\\u2600-\\u2B55\"\n",
        "                        u\"\\u200d\"\n",
        "                        u\"\\u23cf\"\n",
        "                        u\"\\u23e9\"\n",
        "                        u\"\\u231a\"\n",
        "                        u\"\\ufe0f\"\n",
        "                        u\"\\u3030\"\n",
        "                    \"]+\", flags=re.UNICODE)\n",
        "    text = emojis_pattern.sub(r'', text)\n",
        "\n",
        "    # Symbols\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "\n",
        "    # Spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eDrGu2Y4eXiO"
      },
      "source": [
        "We now apply the function to the columns title and content. We leave the columns url and tags untouched because:\n",
        "- **URL**: This doesn't add any information regarding the trend analysis but the url will not work anymore once the symbols are removed.\n",
        "- **Tags**: If we remove all symbols at once, two words that are one tag (like \"Machine Learning\") will be seperated from one another. So we have to normalize it separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aYHi7uxre0Oc",
        "outputId": "f7912c89-d3c9-4b6b-add5-dd0da3889c24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Unnamed: 0                                              title       date  \\\n",
            "0        88571  ricoh to provide customer support for agility ... 2024-09-11   \n",
            "26       70996  ai design trends to watch in 2025 from photore... 2024-09-11   \n",
            "27      104159  ais generate more novel and exciting research ... 2024-09-11   \n",
            "28       73625                argonne s hpc ai user forum wrap up 2024-09-11   \n",
            "29       80150        top 10 supply chain optimisation strategies 2024-09-11   \n",
            "\n",
            "                                              content              domain  \\\n",
            "0   the digit humanoid could work in distribution ...      therobotreport   \n",
            "26  artificial intelligence ai is rapidly transfor...       geeksforgeeks   \n",
            "27  the first statistically significant results ar...            newatlas   \n",
            "28  as fans of this publication will already know ...             hpcwire   \n",
            "29  supply chain optimisation has never been more ...  supplychaindigital   \n",
            "\n",
            "                                                  url  \\\n",
            "0   https://www.therobotreport.com/ricoh-provides-...   \n",
            "26  https://www.geeksforgeeks.org/ai-design-trends...   \n",
            "27  https://newatlas.com/technology/llm-novel-rese...   \n",
            "28  https://www.hpcwire.com/2024/09/11/argonnes-hp...   \n",
            "29  https://www.supplychaindigital.com/top10/top-1...   \n",
            "\n",
            "                                                 tags  \n",
            "0             ['Robotics', 'Video', 'BostonDynamics']  \n",
            "26  ['Personalisation', 'NaturalLanguageProcessing...  \n",
            "27  ['LargeLanguageModel', 'ChatGPT', 'Anthropic',...  \n",
            "28  ['HighPerformanceComputing', 'GenerativeAI', '...  \n",
            "29  ['Finetuning', 'Traceable', 'Accountability', ...  \n"
          ]
        }
      ],
      "source": [
        "df_clean = df_unique.copy()\n",
        "\n",
        "cols_to_clean = ['title', 'content']\n",
        "\n",
        "for col in cols_to_clean:\n",
        "    df_clean[col] = df_clean[col].apply(clean_text)\n",
        "\n",
        "print(df_clean.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bwkl3v1i6ff"
      },
      "source": [
        "We quickly check if there are empty strings now, because there might have been content or titles only consisting of symbols."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3aGP9nQi23u",
        "outputId": "cde57ef5-bc03-4ec3-9b14-3e14ba2e8f9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Empty Values after Cleaning: 0\n"
          ]
        }
      ],
      "source": [
        "empty_after_cleaning = (df_clean[cols_to_clean] == '')\n",
        "print(\"Empty Values after Cleaning:\", empty_after_cleaning.sum().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ_iSuwZiGq3"
      },
      "source": [
        "Now we handle the Tags separately: Lower casing and transform to list instead of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADm1_OVwiazw",
        "outputId": "5ddcbd76-fd25-4fab-92ef-bca33d71d735"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0                     [robotics, video, bostondynamics]\n",
            "26    [personalisation, naturallanguageprocessing, c...\n",
            "27    [largelanguagemodel, chatgpt, anthropic, claud...\n",
            "28    [highperformancecomputing, generativeai, robot...\n",
            "29    [finetuning, traceable, accountability, roboti...\n",
            "Name: tags, dtype: object\n"
          ]
        }
      ],
      "source": [
        "df_clean_tag = df_clean.copy()\n",
        "\n",
        "# Transform strings into real Python lists\n",
        "df_clean_tag['tags'] = df_clean_tag['tags'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# 3. Explode the list, normalize the text, and group it back\n",
        "df_clean_tag['tags'] = (\n",
        "    df_clean_tag['tags']\n",
        "    .explode()\n",
        "    .str.lower()\n",
        "    .str.strip()\n",
        "    .groupby(level=0)\n",
        "    .apply(lambda x: [tag for tag in x.tolist() if pd.notna(tag)])\n",
        ")\n",
        "\n",
        "# check\n",
        "print(df_clean_tag['tags'].head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7e2gg5ylViZ"
      },
      "source": [
        "Finally, give the Dataframe a short name for further analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "8GOwTAzQlb2F"
      },
      "outputs": [],
      "source": [
        "df_final = df_clean_tag.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_S_jcxulpbl"
      },
      "source": [
        "## 1.2 Text Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yk5G5Q-XlvSR"
      },
      "source": [
        "*Done by: XXX  - Checked by: XXX*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rN2hrQiAPjS"
      },
      "source": [
        "### Tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I48UVxJJAxpU"
      },
      "source": [
        "For tokenization, we use spaCy. We only tokenize those columns, that contain text, namely title and content. We do not tokenize the other columns:\n",
        "\n",
        "\n",
        "*   **Date**: Is not in text format.\n",
        "*   **Domain**: This is a categorical identifier and does not add value to the analysis of the content.\n",
        "*   **URL**: This would destroy the functionality of the url.\n",
        "*   **Tags**: We already transformed the tags into a list of strings. Therefore, they are technically already tokenized in a way that respects the specific terminology. If we actually tokenized these values, we would separate key words belonging together like \"machine\" \"learning\".\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "IjBCDmYVA1sl"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "cols_to_tokenize = ['title', 'content']\n",
        "\n",
        "df_token = df_final.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4m1XRuvQ9Bw"
      },
      "source": [
        "Because regular tokenization with spacy took a very long time (we interrupted after 25 minutes), we first asked Gemini for a way to reduce the time. \n",
        "- Using nlp.pipe as well as turning off tagger, parser, ner and lemmatizer were suggested, resulting in a processing time of approx. 28 minutes.\n",
        "- And then we asked Claude and current solution is using nlp.tokenizer.pipe() - purely rule-based (no neural network) tokenizer\n",
        "- And afterwards we decided to implement caching mechanism - saving df_token.pkl to save time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b46qHH-JQyiX",
        "outputId": "00d78c4f-3a56-403a-a802-b0c4bbf84ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizing title...\n",
            "Tokenizing content...\n",
            "Tokenization complete!\n",
            "Lemmatizing title...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatizing content...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/aptt@mediait.ch/miniconda3/envs/RS/lib/python3.11/site-packages/spacy/pipeline/lemmatizer.py:188: UserWarning: [W108] The rule-based lemmatizer did not find POS annotation for one or more tokens. Check that your pipeline includes components that assign token.pos, typically 'tagger'+'attribute_ruler' or 'morphologizer'.\n",
            "  warnings.warn(Warnings.W108)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemmatization complete!\n",
            "✅ Saved to cache: data/cache/df_token.pkl\n"
          ]
        }
      ],
      "source": [
        "# Caching mechanism, to avoid re-running the expensive lemmatization step every time\n",
        "if ENV == \"local\":\n",
        "    CACHE_PATH = \"data/cache/df_token.pkl\"\n",
        "else:\n",
        "    CACHE_PATH = \"/content/drive/My Drive/CLT/cache/df_token.pkl\"\n",
        "\n",
        "os.makedirs(os.path.dirname(CACHE_PATH), exist_ok=True)\n",
        "\n",
        "expected_cols = [f'{col}_{kind}' for col in cols_to_tokenize for kind in ('token', 'lemma')]\n",
        "\n",
        "if os.path.exists(CACHE_PATH):\n",
        "    _cached = pd.read_pickle(CACHE_PATH)\n",
        "    if all(c in _cached.columns for c in expected_cols):\n",
        "        df_token = _cached\n",
        "        print(\"⚡ Loaded tokenization & lemmatization from cache!\")\n",
        "    else:\n",
        "        print(\"⚠️ Cache incomplete, re-running...\")\n",
        "        os.remove(CACHE_PATH)\n",
        "\n",
        "if not all(c in df_token.columns for c in expected_cols):\n",
        "    # --- Tokenization ---\n",
        "    # nlp.tokenizer.pipe() is purely rule-based — no neural network involved\n",
        "    for col in cols_to_tokenize:\n",
        "        print(f\"Tokenizing {col}...\")\n",
        "        df_token[f'{col}_token'] = [\n",
        "            [token.text for token in doc]\n",
        "            for doc in nlp.tokenizer.pipe(df_token[col].astype(str), batch_size=1000)\n",
        "        ]\n",
        "    print(\"Tokenization complete!\")\n",
        "\n",
        "    # --- Lemmatization ---\n",
        "    # Disable tok2vec + tagger to use rule/lookup lemmatization only (no neural net).\n",
        "    # Slightly less accurate than POS-informed lemmas but orders of magnitude faster\n",
        "    # and sufficient for trend analysis (e.g. \"running\" → \"run\", \"companies\" → \"company\").\n",
        "    for col in cols_to_tokenize:\n",
        "        print(f\"Lemmatizing {col}...\")\n",
        "        df_token[f'{col}_lemma'] = [\n",
        "            [token.lemma_ for token in doc]\n",
        "            for doc in nlp.pipe(\n",
        "                df_token[col].astype(str),\n",
        "                batch_size=1000,\n",
        "                disable=[\"tok2vec\", \"tagger\", \"parser\", \"ner\"]\n",
        "            )\n",
        "        ]\n",
        "    print(\"Lemmatization complete!\")\n",
        "\n",
        "    df_token.to_pickle(CACHE_PATH)\n",
        "    print(f\"✅ Saved to cache: {CACHE_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "collapsed": true,
        "id": "k_2nRncYZaCi"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Unnamed: 0                                              title       date  \\\n",
            "0        88571  ricoh to provide customer support for agility ... 2024-09-11   \n",
            "26       70996  ai design trends to watch in 2025 from photore... 2024-09-11   \n",
            "27      104159  ais generate more novel and exciting research ... 2024-09-11   \n",
            "28       73625                argonne s hpc ai user forum wrap up 2024-09-11   \n",
            "29       80150        top 10 supply chain optimisation strategies 2024-09-11   \n",
            "\n",
            "                                              content              domain  \\\n",
            "0   the digit humanoid could work in distribution ...      therobotreport   \n",
            "26  artificial intelligence ai is rapidly transfor...       geeksforgeeks   \n",
            "27  the first statistically significant results ar...            newatlas   \n",
            "28  as fans of this publication will already know ...             hpcwire   \n",
            "29  supply chain optimisation has never been more ...  supplychaindigital   \n",
            "\n",
            "                                                  url  \\\n",
            "0   https://www.therobotreport.com/ricoh-provides-...   \n",
            "26  https://www.geeksforgeeks.org/ai-design-trends...   \n",
            "27  https://newatlas.com/technology/llm-novel-rese...   \n",
            "28  https://www.hpcwire.com/2024/09/11/argonnes-hp...   \n",
            "29  https://www.supplychaindigital.com/top10/top-1...   \n",
            "\n",
            "                                                 tags  \\\n",
            "0                   [robotics, video, bostondynamics]   \n",
            "26  [personalisation, naturallanguageprocessing, c...   \n",
            "27  [largelanguagemodel, chatgpt, anthropic, claud...   \n",
            "28  [highperformancecomputing, generativeai, robot...   \n",
            "29  [finetuning, traceable, accountability, roboti...   \n",
            "\n",
            "                                          title_token  \\\n",
            "0   [ricoh, to, provide, customer, support, for, a...   \n",
            "26  [ai, design, trends, to, watch, in, 2025, from...   \n",
            "27  [ais, generate, more, novel, and, exciting, re...   \n",
            "28       [argonne, s, hpc, ai, user, forum, wrap, up]   \n",
            "29  [top, 10, supply, chain, optimisation, strateg...   \n",
            "\n",
            "                                        content_token  \\\n",
            "0   [the, digit, humanoid, could, work, in, distri...   \n",
            "26  [artificial, intelligence, ai, is, rapidly, tr...   \n",
            "27  [the, first, statistically, significant, resul...   \n",
            "28  [as, fans, of, this, publication, will, alread...   \n",
            "29  [supply, chain, optimisation, has, never, been...   \n",
            "\n",
            "                                          title_lemma  \\\n",
            "0   [ricoh, to, provide, customer, support, for, a...   \n",
            "26  [ai, design, trends, to, watch, in, 2025, from...   \n",
            "27  [ais, generate, more, novel, and, exciting, re...   \n",
            "28       [argonne, s, hpc, ai, user, forum, wrap, up]   \n",
            "29  [top, 10, supply, chain, optimisation, strateg...   \n",
            "\n",
            "                                        content_lemma  \n",
            "0   [the, digit, humanoid, could, work, in, distri...  \n",
            "26  [artificial, intelligence, ai, is, rapidly, tr...  \n",
            "27  [the, first, statistically, significant, resul...  \n",
            "28  [as, fans, of, this, publication, will, alread...  \n",
            "29  [supply, chain, optimisation, has, never, been...  \n"
          ]
        }
      ],
      "source": [
        "print(df_token.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aA3Sw1NgZiyX"
      },
      "source": [
        "### Lemmatization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92_6uj0IZ21b"
      },
      "source": [
        "For this, we equally asked Gemini for a time efficient method. It took approx. 35 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "GkReRxAXZllW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Lemma columns available: ['title_lemma', 'content_lemma']\n"
          ]
        }
      ],
      "source": [
        "# Lemmatization is handled in the tokenization cell above (single cache block)\n",
        "print(\"Lemma columns available:\", [c for c in df_token.columns if 'lemma' in c])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Stop word removal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "mXACyp109025"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stop word removal complete!\n"
          ]
        }
      ],
      "source": [
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "stop_words = STOP_WORDS\n",
        "df_sw = df_token.copy()\n",
        "cols_to_process = ['title', 'content']\n",
        "\n",
        "for col in cols_to_process:\n",
        "    # Remove stop words from tokenized columns\n",
        "    df_sw[f'{col}_token_sw'] = df_sw[f'{col}_token'].apply(\n",
        "        lambda tokens: [t for t in tokens if t.lower() not in stop_words]\n",
        "    )\n",
        "    # Remove stop words from lemmatized columns\n",
        "    df_sw[f'{col}_lemma_sw'] = df_sw[f'{col}_lemma'].apply(\n",
        "        lambda tokens: [t for t in tokens if t.lower() not in stop_words]\n",
        "    )\n",
        "\n",
        "print(\"Stop word removal complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== TITLE ===\n",
            "Original tokens:        ['argonne', 's', 'hpc', 'ai', 'user', 'forum', 'wrap', 'up']\n",
            "Tokens (no stopwords):  ['argonne', 's', 'hpc', 'ai', 'user', 'forum', 'wrap']\n",
            "Original lemmas:        ['argonne', 's', 'hpc', 'ai', 'user', 'forum', 'wrap', 'up']\n",
            "Lemmas (no stopwords):  ['argonne', 's', 'hpc', 'ai', 'user', 'forum', 'wrap']\n",
            "\n",
            "=== CONTENT (first 15 items) ===\n",
            "Original tokens:        ['as', 'fans', 'of', 'this', 'publication', 'will', 'already', 'know', 'ai', 'is', 'everywhere', 'we', 'hear', 'about', 'it']\n",
            "Tokens (no stopwords):  ['fans', 'publication', 'know', 'ai', 'hear', 'news', 'work', 'daily', 'lives', 's', 'revolutionary', 'technology', 'established', 'events', 'focusing']\n",
            "Original lemmas:        ['as', 'fans', 'of', 'this', 'publication', 'will', 'already', 'know', 'ai', 'is', 'everywhere', 'we', 'hear', 'about', 'it']\n",
            "Lemmas (no stopwords):  ['fans', 'publication', 'know', 'ai', 'hear', 'news', 'work', 'daily', 'lives', 's', 'revolutionary', 'technology', 'established', 'events', 'focusing']\n"
          ]
        }
      ],
      "source": [
        "# Verify: compare original vs filtered (title)\n",
        "print(\"=== TITLE ===\")\n",
        "print(\"Original tokens:       \", df_sw['title_token'].iloc[3])\n",
        "print(\"Tokens (no stopwords): \", df_sw['title_token_sw'].iloc[3])\n",
        "print(\"Original lemmas:       \", df_sw['title_lemma'].iloc[3])\n",
        "print(\"Lemmas (no stopwords): \", df_sw['title_lemma_sw'].iloc[3])\n",
        "\n",
        "print()\n",
        "\n",
        "# Content is longer and shows the difference between tokens and lemmas more clearly\n",
        "print(\"=== CONTENT (first 15 items) ===\")\n",
        "print(\"Original tokens:       \", df_sw['content_token'].iloc[3][:15])\n",
        "print(\"Tokens (no stopwords): \", df_sw['content_token_sw'].iloc[3][:15])\n",
        "print(\"Original lemmas:       \", df_sw['content_lemma'].iloc[3][:15])\n",
        "print(\"Lemmas (no stopwords): \", df_sw['content_lemma_sw'].iloc[3][:15])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Consistent Representation\n",
        "\n",
        "We apply three normalization steps to the token and lemma lists:\n",
        "\n",
        "- **Acronym standardization**: A domain-specific dictionary maps variant forms to a canonical token (e.g. `\"a.i.\"` → `\"ai\"`, `\"llms\"` → `\"llm\"`). This ensures the same concept is not counted as multiple distinct terms.\n",
        "- **Abbreviation expansion**: Common abbreviations are expanded to their full written form (e.g. `\"govt\"` → `\"government\"`).\n",
        "- **Noise filtering**: Pure numeric tokens and single-character tokens (except meaningful ones like `\"r\"` or `\"c\"` for programming languages) are removed, as they carry no semantic value for trend analysis.\n",
        "\n",
        "> **Note on contractions** (`don't` → `do not`): The `clean_text` function already removed apostrophes, so contractions were split into noise tokens (e.g. `\"don\"`, `\"t\"`) earlier in the pipeline. The noise filter below removes these fragments. In a future re-run, a contraction expansion step (e.g. using the `contractions` library) should be added *before* `clean_text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Acronym & abbreviation dictionary (domain: AI / tech media) ---\n",
        "NORM_DICT = {\n",
        "    # Acronym variants → canonical form\n",
        "    \"a.i.\": \"ai\",\n",
        "    \"ml\":   \"machine learning\",\n",
        "    \"nlp\":  \"natural language processing\",\n",
        "    \"llm\":  \"large language model\",\n",
        "    \"llms\": \"large language model\",\n",
        "    \"gpt\":  \"gpt\",\n",
        "    \"cv\":   \"computer vision\",\n",
        "    \"rl\":   \"reinforcement learning\",\n",
        "    \"dl\":   \"deep learning\",\n",
        "    \"nn\":   \"neural network\",\n",
        "    \"nns\":  \"neural network\",\n",
        "    # Common abbreviations\n",
        "    \"govt\": \"government\",\n",
        "    \"corp\": \"corporation\",\n",
        "    \"dept\": \"department\",\n",
        "    \"approx\": \"approximately\",\n",
        "    \"vs\":   \"versus\",\n",
        "    \"etc\":  \"etcetera\",\n",
        "    \"eg\":   \"for example\",\n",
        "    \"ie\":   \"that is\",\n",
        "    # Noise left by contraction splitting\n",
        "    \"n\":    None,   # None = remove the token\n",
        "    \"t\":    None,\n",
        "    \"ve\":   None,\n",
        "    \"re\":   None,\n",
        "    \"ll\":   None,\n",
        "    \"s\":    None,\n",
        "    \"d\":    None,\n",
        "}\n",
        "\n",
        "# Tokens to keep even though they are single characters (programming languages, etc.)\n",
        "KEEP_SINGLE_CHARS = {\"r\", \"c\", \"q\"}\n",
        "\n",
        "def normalize_tokens(tokens):\n",
        "    result = []\n",
        "    for token in tokens:\n",
        "        t = token.lower()\n",
        "        # Apply dictionary mapping\n",
        "        if t in NORM_DICT:\n",
        "            replacement = NORM_DICT[t]\n",
        "            if replacement is None:\n",
        "                continue                    # drop the token\n",
        "            result.append(replacement)\n",
        "        # Drop pure numbers\n",
        "        elif t.isdigit():\n",
        "            continue\n",
        "        # Drop single characters unless explicitly kept\n",
        "        elif len(t) == 1 and t not in KEEP_SINGLE_CHARS:\n",
        "            continue\n",
        "        else:\n",
        "            result.append(t)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "eWpGVTVD-FO9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Normalization complete!\n",
            "\n",
            "=== TITLE ===\n",
            "Before: ['argonne', 's', 'hpc', 'ai', 'user', 'forum', 'wrap']\n",
            "After:  ['argonne', 'hpc', 'ai', 'user', 'forum', 'wrap']\n",
            "\n",
            "=== CONTENT (first 15 items) ===\n",
            "Before: ['fans', 'publication', 'know', 'ai', 'hear', 'news', 'work', 'daily', 'lives', 's', 'revolutionary', 'technology', 'established', 'events', 'focusing']\n",
            "After:  ['fans', 'publication', 'know', 'ai', 'hear', 'news', 'work', 'daily', 'lives', 'revolutionary', 'technology', 'established', 'events', 'focusing', 'hpc']\n"
          ]
        }
      ],
      "source": [
        "df_norm = df_sw.copy()\n",
        "\n",
        "cols_to_normalize = ['title', 'content']\n",
        "\n",
        "for col in cols_to_normalize:\n",
        "    df_norm[f'{col}_token_norm'] = df_norm[f'{col}_token_sw'].apply(normalize_tokens)\n",
        "    df_norm[f'{col}_lemma_norm'] = df_norm[f'{col}_lemma_sw'].apply(normalize_tokens)\n",
        "\n",
        "print(\"Normalization complete!\")\n",
        "\n",
        "# Verify\n",
        "print(\"\\n=== TITLE ===\")\n",
        "print(\"Before:\", df_norm['title_token_sw'].iloc[3])\n",
        "print(\"After: \", df_norm['title_token_norm'].iloc[3])\n",
        "\n",
        "print(\"\\n=== CONTENT (first 15 items) ===\")\n",
        "print(\"Before:\", df_norm['content_lemma_sw'].iloc[3][:15])\n",
        "print(\"After: \", df_norm['content_lemma_norm'].iloc[3][:15])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpuXJlAsno2f"
      },
      "source": [
        "## 1.3 EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJGG_hMCnraP"
      },
      "source": [
        "*Done by: XXX  - Checked by: XXX*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKZMrS57nxFC"
      },
      "source": [
        "## 1.4 Entity and Relationship Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggtoQsHBn0gi"
      },
      "source": [
        "*Done by: XXX  - Checked by: XXX*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySxUMzBAn5jx"
      },
      "source": [
        "## 1.5 Knowledge Graph Construction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XKqNYaHn8gE"
      },
      "source": [
        "*Done by: XXX  - Checked by: XXX*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEQyUAaln9U1"
      },
      "source": [
        "## 1.6 Topic Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iwhxsxGIoBYu"
      },
      "source": [
        "*Done by: XXX  - Checked by: XXX*"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "RS",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
